{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Homework 8: NLP & NLTK - sentiment analysis on movie reviews\n",
    "#### Using Natural Language Processing in Python\n",
    "\n",
    "Source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "\n",
    "*Code snippets given run on both Python 2.7 and Python 3.5*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Topics covered:\n",
    "\n",
    "* Data exploration\n",
    "* Text preprocessing\n",
    "* Stemming\n",
    "* Part of Speech (POS) tagging\n",
    "* Lemmatizing\n",
    "* Stopwords (abundant words)\n",
    "* Bag of Words + Feature extraction\n",
    "* Sentiment prediction using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Pre-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division, absolute_import #make compatible with Python 2 and Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data description\n",
    "\n",
    "You can download the data (labeledTrainData.tsv.zip) here: https://www.kaggle.com/c/word2vec-nlp-tutorial/data, place it in your working directory & unzip the file.\n",
    "\n",
    "## Data set\n",
    "\n",
    "The labeled data set consists of 25,000 IMDB movie reviews. The sentiment of reviews is binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for Â£1, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"2486_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What happens when an army of wetbacks, towelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"6811_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Although I generally do not like remakes beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"11744_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"7369_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I had a feeling that after \\\"Submerged\\\", thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"12081_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"note to George Litman, and others: the Myster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"3561_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Stephen King adaptation (scripted by King him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"4489_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"`The Matrix' was an exciting summer blockbust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"3951_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Ulli Lommel's 1980 film 'The Boogey Man' is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"3304_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This movie is one among the very few Indian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"9352_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Most people, especially young people, may not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"3374_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Soylent Green\\\" is one of the best and most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"10782_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Michael Stearns plays Mike, a sexually frustr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"5414_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This happy-go-luck 1939 military swashbuckler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"10492_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I would love to have that two hours of my lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"3350_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The script for this movie was probably found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"6581_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Looking for Quo Vadis at my local video store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"2203_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Note to all mad scientists everywhere: if you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"689_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What the ........... is this ? This must, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"9152_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Intrigued by the synopsis (every gay video th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"6077_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Would anyone really watch this RUBBISH if it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>\"9389_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Red Rock West (1993)&lt;br /&gt;&lt;br /&gt;Nicolas Cage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>\"9251_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"what can i say?, ms Erika Eleniak is my favor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>\"1422_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The spoiler warning is for those people who w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>\"7415_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What do you call a horror story without horro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>\"7492_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Though not a horror film in the traditional s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>\"7689_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This was what black society was like before t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>\"12370_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"They probably should have called this movie T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>\"5625_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Attractive Marjorie(Farrah Fawcett)lives in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>\"9397_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Vaguely reminiscent of great 1940's westerns,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>\"5992_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I admit I had no idea what to expect before v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>\"2488_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"To me, the final scene, in which Harris respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>\"9627_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This is by far the funniest short made by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>\"3822_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"To be a Buster Keaton fan is to have your hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>\"5983_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I was one of those \\\"few Americans\\\" that gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>\"8021_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Visually disjointed and full of itself, the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>\"3471_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this movie had more holes than a piece of swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>\"6034_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Last November, I had a chance to see this fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>\"1988_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"First off, I'd like to make a correction on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>\"7623_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"While originally reluctant to jump on the ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>\"5974_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I heard about this movie when watching VH1's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>\"2034_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I've never been huge on IMAX films. They're c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>\"9416_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Steve McQueen has certainly a lot of loyal fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>\"10994_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Sometimes you wonder how some people get fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>\"10957_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I am a student of film, and have been for sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>\"2372_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Unimaginably stupid, redundant and humiliatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"3453_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It seems like more consideration has gone int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"5064_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I don't believe they made this film. Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"10905_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Guy is a loser. Can't get girls, needs to bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"10194_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This 30 minute documentary BuÃ±uel made in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"8478_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I saw this movie as a child and it broke my h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  sentiment                                             review\n",
       "0       \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1       \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2       \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3       \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4       \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5       \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
       "6       \"7166_2\"          0  \"This movie could have been very good, but com...\n",
       "7      \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
       "8        \"319_1\"          0  \"A friend of mine bought this film for Â£1, and...\n",
       "9      \"8713_10\"          1  \"<br /><br />This movie is full of references....\n",
       "10      \"2486_3\"          0  \"What happens when an army of wetbacks, towelh...\n",
       "11     \"6811_10\"          1  \"Although I generally do not like remakes beli...\n",
       "12     \"11744_9\"          1  \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...\n",
       "13      \"7369_1\"          0  \"I had a feeling that after \\\"Submerged\\\", thi...\n",
       "14     \"12081_1\"          0  \"note to George Litman, and others: the Myster...\n",
       "15      \"3561_4\"          0  \"Stephen King adaptation (scripted by King him...\n",
       "16      \"4489_1\"          0  \"`The Matrix' was an exciting summer blockbust...\n",
       "17      \"3951_2\"          0  \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18     \"3304_10\"          1  \"This movie is one among the very few Indian m...\n",
       "19     \"9352_10\"          1  \"Most people, especially young people, may not...\n",
       "20      \"3374_7\"          1  \"\\\"Soylent Green\\\" is one of the best and most...\n",
       "21     \"10782_7\"          1  \"Michael Stearns plays Mike, a sexually frustr...\n",
       "22     \"5414_10\"          1  \"This happy-go-luck 1939 military swashbuckler...\n",
       "23     \"10492_1\"          0  \"I would love to have that two hours of my lif...\n",
       "24      \"3350_3\"          0  \"The script for this movie was probably found ...\n",
       "25      \"6581_7\"          1  \"Looking for Quo Vadis at my local video store...\n",
       "26      \"2203_3\"          0  \"Note to all mad scientists everywhere: if you...\n",
       "27       \"689_1\"          0  \"What the ........... is this ? This must, wit...\n",
       "28      \"9152_1\"          0  \"Intrigued by the synopsis (every gay video th...\n",
       "29      \"6077_1\"          0  \"Would anyone really watch this RUBBISH if it ...\n",
       "...          ...        ...                                                ...\n",
       "24970   \"9389_7\"          1  \"Red Rock West (1993)<br /><br />Nicolas Cage ...\n",
       "24971   \"9251_9\"          1  \"what can i say?, ms Erika Eleniak is my favor...\n",
       "24972  \"1422_10\"          1  \"The spoiler warning is for those people who w...\n",
       "24973   \"7415_2\"          0  \"What do you call a horror story without horro...\n",
       "24974   \"7492_7\"          1  \"Though not a horror film in the traditional s...\n",
       "24975  \"7689_10\"          1  \"This was what black society was like before t...\n",
       "24976  \"12370_4\"          0  \"They probably should have called this movie T...\n",
       "24977   \"5625_8\"          1  \"Attractive Marjorie(Farrah Fawcett)lives in f...\n",
       "24978   \"9397_9\"          1  \"Vaguely reminiscent of great 1940's westerns,...\n",
       "24979   \"5992_7\"          1  \"I admit I had no idea what to expect before v...\n",
       "24980  \"2488_10\"          1  \"To me, the final scene, in which Harris respo...\n",
       "24981  \"9627_10\"          1  \"This is by far the funniest short made by the...\n",
       "24982   \"3822_2\"          0  \"To be a Buster Keaton fan is to have your hea...\n",
       "24983   \"5983_4\"          0  \"I was one of those \\\"few Americans\\\" that gre...\n",
       "24984   \"8021_2\"          0  \"Visually disjointed and full of itself, the d...\n",
       "24985   \"3471_3\"          0  \"this movie had more holes than a piece of swi...\n",
       "24986  \"6034_10\"          1  \"Last November, I had a chance to see this fil...\n",
       "24987   \"1988_9\"          1  \"First off, I'd like to make a correction on a...\n",
       "24988   \"7623_9\"          1  \"While originally reluctant to jump on the ban...\n",
       "24989   \"5974_7\"          1  \"I heard about this movie when watching VH1's ...\n",
       "24990   \"2034_9\"          1  \"I've never been huge on IMAX films. They're c...\n",
       "24991   \"9416_3\"          0  \"Steve McQueen has certainly a lot of loyal fa...\n",
       "24992  \"10994_1\"          0  \"Sometimes you wonder how some people get fund...\n",
       "24993  \"10957_3\"          0  \"I am a student of film, and have been for sev...\n",
       "24994   \"2372_1\"          0  \"Unimaginably stupid, redundant and humiliatin...\n",
       "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
       "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
       "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
       "24998  \"10194_3\"          0  \"This 30 minute documentary BuÃ±uel made in the...\n",
       "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data to a Pandas data frame\n",
    "# Use header = 0 (first line contains col names)\n",
    "# use delimiter=\\t (columns are separated by tabs),\n",
    "# use quoting=3 (Python will ignore doubled quotes)\n",
    "\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "train.shape == (25000,3)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Q1.1:\n",
    "* 1: How many movie reviews are positive and how many are negative in labeledTrainData.tsv?\n",
    "\n",
    "\n",
    "* 2: What is the average length of all the reviews (string length)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 12500\n",
      "Number of negative reviews: 12500\n",
      "Average lenght of all the reviews: 1329.71056\n"
     ]
    }
   ],
   "source": [
    "## Input answer ##\n",
    "\n",
    "#1.\n",
    "pos_rev = len(train[train['sentiment']==1])\n",
    "neg_rev = len(train[train['sentiment']==0])\n",
    "print('Number of positive reviews:', pos_rev)\n",
    "print('Number of negative reviews:', neg_rev)\n",
    "\n",
    "#2.\n",
    "avg_len_rev = np.mean([(len(rev)) for rev in train['review']])\n",
    "print('Average lenght of all the reviews:', avg_len_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Q1:2 Explore NLP on one review\n",
    "\n",
    "In Q1:2 you should work on the third review, i.e. `train['review'][2]`\n",
    "\n",
    "First we would like to clean up the reviews. As you can see many interviews contain \\ characters in front of quotation symobols, \"`<br/>` tags, numbers, abbrevations etc.\n",
    "\n",
    "* 1: Remove all the HTML tags in the third review, by creating a beatifulsoup object and then using the `.text` method. Save results in variable `review3`\n",
    "\n",
    "\n",
    "* 2: Import NLTK's sent_tokenizer and count the number of sentences in review 3 (cleaned from HTML tags). To import sent_tokenizer use: `from nltk.tokenize import sent_tokenize`\n",
    "\n",
    "\n",
    "* 3: Remove all punctuation, numbers and special characters from the third review (cleaned from HTML tags). We can do this using Regular Expression - package `re`. Save the results in variable `review3`. Code given to you as we haven't covered this in class:\n",
    "\n",
    "        review3 = re.sub('[^a-zA-Z]',' ',review3)\n",
    "\n",
    "\n",
    "* 4: Convert all the letters to lower case (save in variable `review3`). Then split the string so that every word is one element in a list (save the list in variable `review3_words`). Note: When we split the strings into words that process is called tokenization.\n",
    "\n",
    "\n",
    "* 5: Use NLTK's PorterStemmer (`from nltk.stem import PorterStemmer`). Create a new Porter stemmer (`stemmer = PorterStemmer()`) and run it on every word in `review3_words`, print the results as one string (don't overwrite the `review3_words` variable from 4). What does the PorterStemmer do?\n",
    "\n",
    "\n",
    "* 6: Now we want to Part Of Speech (POS)) tag the third movie review. We will use POS labeling also called grammatical tagging. To do this import `from nltk.tag import pos_tag`. When you use pos_tag on a word it returns a token-tag pair in the form of a tuple. In NLTK's Penn Treebank POS, the abbreviation (tag) for an adjective is JJ and NN for singular nouns. Count the number of singular nouns (NN) and adjectives (JJ) in `review3_words` using NLTK's pos_tag_sents. A list of the Penn Treebank pos_tag's can be found here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "\n",
    "* 7: An even more sophisticated operation than stemming using the PorterStemmer is called lemmatizing. Lemmatizing, in contrast to stemming, does not create non-existent words and converts words to their synonyms. In order to use lemmatizing we need to define the wordnet POS tag. A function that takes in a POS Penn Treebank tag and converts it to a wordnet tag and then lemmatizes words in a string has been given to you below. Please extend this code and print the lemmatized third movie review. Don't save the results in any variable.\n",
    "\n",
    "\n",
    "* 8: Lastly we will try to remove common words that don't carry much information. These are called stopwords. In English they could for example be 'am', 'are', 'and' etc. To import NLTK's list of stopwords you need to download the stopword corpora (`import nltk` and then `nltk.download()` if you don't have it). When that is done run `from nltk.corpus import stopwords` and create a variable for English stopwords with `eng_stopwords = stopwords.words('english')`. Use the list of English stopwords to remove all the stopwords from your list of words in the third movie review, i.e. `review3_words`. Print `review3_words` without stopwords, count the number of stopwords removed and print them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like Â¨Jurassik ParkÂ¨, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : Â¨Sabretooth(2002)Â¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better Â¨10.000 BC(2006)Â¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "review3 = train['review'][2] # the review used for analysis in Q1:2\n",
    "review3 = bs.BeautifulSoup(review3, features='lxml').text\n",
    "review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in review3: 13\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = nltk.tokenize.sent_tokenize(text=review3)\n",
    "num_sent = len(sentences)\n",
    "print('Number of sentences in review3:', num_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The film starts with a manager  Nicholas Bell  giving welcome investors  Robert Carradine  to Primal Park   A secret project mutating a primal animal using fossilized DNA  like  Jurassik Park   and some scientists resurrect one of nature s most fearsome predators  the Sabretooth tiger or Smilodon   Scientific ambition turns deadly  however  and when the high voltage fence is opened the creature escape and begins savagely stalking its prey   the human visitors   tourists and scientific Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre historical animals which are deadlier and bigger   In addition   a security agent  Stacy Haiduk  and her mate  Brian Wimmer  fight hardly against the carnivorous Smilodons  The Sabretooths  themselves   of course  are the real star stars and they are astounding terrifyingly though not convincing  The giant animals savagely are stalking its prey and the group run afoul and fight against one nature s most fearsome predators  Furthermore a third Sabretooth more dangerous and slow stalks its victims The movie delivers the goods with lots of blood and gore as beheading  hair raising chills full of scares when the Sabretooths appear with mediocre special effects The story provides exciting and stirring entertainment but it results to be quite boring  The giant animals are majority made by computer generator and seem totally lousy  Middling performances though the players reacting appropriately to becoming food Actors give vigorously physical performances dodging the beasts  running bound and leaps or dangling over walls   And it packs a ridiculous final deadly scene  No for small kids by realistic gory and violent attack scenes   Other films about Sabretooths or Smilodon are the following    Sabretooth       by James R Hickox with Vanessa Angel  David Keith and John Rhys Davies and the much better         BC        by Roland Emmerich with with Steven Strait  Cliff Curtis and Camilla Belle  This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films  Miller is an Australian director usually working for television  Tidal wave  Journey to the center of the earth  and many others  and occasionally for cinema   The man from Snowy river  Zeus and Roxanne Robinson Crusoe    Rating   Below average  bottom of barrel  '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "review3 = re.sub('[^a-zA-Z]',' ',review3)\n",
    "review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'film',\n",
       " 'starts',\n",
       " 'with',\n",
       " 'a',\n",
       " 'manager',\n",
       " 'nicholas',\n",
       " 'bell',\n",
       " 'giving',\n",
       " 'welcome',\n",
       " 'investors',\n",
       " 'robert',\n",
       " 'carradine',\n",
       " 'to',\n",
       " 'primal',\n",
       " 'park',\n",
       " 'a',\n",
       " 'secret',\n",
       " 'project',\n",
       " 'mutating',\n",
       " 'a',\n",
       " 'primal',\n",
       " 'animal',\n",
       " 'using',\n",
       " 'fossilized',\n",
       " 'dna',\n",
       " 'like',\n",
       " 'jurassik',\n",
       " 'park',\n",
       " 'and',\n",
       " 'some',\n",
       " 'scientists',\n",
       " 'resurrect',\n",
       " 'one',\n",
       " 'of',\n",
       " 'nature',\n",
       " 's',\n",
       " 'most',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'the',\n",
       " 'sabretooth',\n",
       " 'tiger',\n",
       " 'or',\n",
       " 'smilodon',\n",
       " 'scientific',\n",
       " 'ambition',\n",
       " 'turns',\n",
       " 'deadly',\n",
       " 'however',\n",
       " 'and',\n",
       " 'when',\n",
       " 'the',\n",
       " 'high',\n",
       " 'voltage',\n",
       " 'fence',\n",
       " 'is',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'creature',\n",
       " 'escape',\n",
       " 'and',\n",
       " 'begins',\n",
       " 'savagely',\n",
       " 'stalking',\n",
       " 'its',\n",
       " 'prey',\n",
       " 'the',\n",
       " 'human',\n",
       " 'visitors',\n",
       " 'tourists',\n",
       " 'and',\n",
       " 'scientific',\n",
       " 'meanwhile',\n",
       " 'some',\n",
       " 'youngsters',\n",
       " 'enter',\n",
       " 'in',\n",
       " 'the',\n",
       " 'restricted',\n",
       " 'area',\n",
       " 'of',\n",
       " 'the',\n",
       " 'security',\n",
       " 'center',\n",
       " 'and',\n",
       " 'are',\n",
       " 'attacked',\n",
       " 'by',\n",
       " 'a',\n",
       " 'pack',\n",
       " 'of',\n",
       " 'large',\n",
       " 'pre',\n",
       " 'historical',\n",
       " 'animals',\n",
       " 'which',\n",
       " 'are',\n",
       " 'deadlier',\n",
       " 'and',\n",
       " 'bigger',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'a',\n",
       " 'security',\n",
       " 'agent',\n",
       " 'stacy',\n",
       " 'haiduk',\n",
       " 'and',\n",
       " 'her',\n",
       " 'mate',\n",
       " 'brian',\n",
       " 'wimmer',\n",
       " 'fight',\n",
       " 'hardly',\n",
       " 'against',\n",
       " 'the',\n",
       " 'carnivorous',\n",
       " 'smilodons',\n",
       " 'the',\n",
       " 'sabretooths',\n",
       " 'themselves',\n",
       " 'of',\n",
       " 'course',\n",
       " 'are',\n",
       " 'the',\n",
       " 'real',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'and',\n",
       " 'they',\n",
       " 'are',\n",
       " 'astounding',\n",
       " 'terrifyingly',\n",
       " 'though',\n",
       " 'not',\n",
       " 'convincing',\n",
       " 'the',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'savagely',\n",
       " 'are',\n",
       " 'stalking',\n",
       " 'its',\n",
       " 'prey',\n",
       " 'and',\n",
       " 'the',\n",
       " 'group',\n",
       " 'run',\n",
       " 'afoul',\n",
       " 'and',\n",
       " 'fight',\n",
       " 'against',\n",
       " 'one',\n",
       " 'nature',\n",
       " 's',\n",
       " 'most',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'furthermore',\n",
       " 'a',\n",
       " 'third',\n",
       " 'sabretooth',\n",
       " 'more',\n",
       " 'dangerous',\n",
       " 'and',\n",
       " 'slow',\n",
       " 'stalks',\n",
       " 'its',\n",
       " 'victims',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'delivers',\n",
       " 'the',\n",
       " 'goods',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'blood',\n",
       " 'and',\n",
       " 'gore',\n",
       " 'as',\n",
       " 'beheading',\n",
       " 'hair',\n",
       " 'raising',\n",
       " 'chills',\n",
       " 'full',\n",
       " 'of',\n",
       " 'scares',\n",
       " 'when',\n",
       " 'the',\n",
       " 'sabretooths',\n",
       " 'appear',\n",
       " 'with',\n",
       " 'mediocre',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'the',\n",
       " 'story',\n",
       " 'provides',\n",
       " 'exciting',\n",
       " 'and',\n",
       " 'stirring',\n",
       " 'entertainment',\n",
       " 'but',\n",
       " 'it',\n",
       " 'results',\n",
       " 'to',\n",
       " 'be',\n",
       " 'quite',\n",
       " 'boring',\n",
       " 'the',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'majority',\n",
       " 'made',\n",
       " 'by',\n",
       " 'computer',\n",
       " 'generator',\n",
       " 'and',\n",
       " 'seem',\n",
       " 'totally',\n",
       " 'lousy',\n",
       " 'middling',\n",
       " 'performances',\n",
       " 'though',\n",
       " 'the',\n",
       " 'players',\n",
       " 'reacting',\n",
       " 'appropriately',\n",
       " 'to',\n",
       " 'becoming',\n",
       " 'food',\n",
       " 'actors',\n",
       " 'give',\n",
       " 'vigorously',\n",
       " 'physical',\n",
       " 'performances',\n",
       " 'dodging',\n",
       " 'the',\n",
       " 'beasts',\n",
       " 'running',\n",
       " 'bound',\n",
       " 'and',\n",
       " 'leaps',\n",
       " 'or',\n",
       " 'dangling',\n",
       " 'over',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'it',\n",
       " 'packs',\n",
       " 'a',\n",
       " 'ridiculous',\n",
       " 'final',\n",
       " 'deadly',\n",
       " 'scene',\n",
       " 'no',\n",
       " 'for',\n",
       " 'small',\n",
       " 'kids',\n",
       " 'by',\n",
       " 'realistic',\n",
       " 'gory',\n",
       " 'and',\n",
       " 'violent',\n",
       " 'attack',\n",
       " 'scenes',\n",
       " 'other',\n",
       " 'films',\n",
       " 'about',\n",
       " 'sabretooths',\n",
       " 'or',\n",
       " 'smilodon',\n",
       " 'are',\n",
       " 'the',\n",
       " 'following',\n",
       " 'sabretooth',\n",
       " 'by',\n",
       " 'james',\n",
       " 'r',\n",
       " 'hickox',\n",
       " 'with',\n",
       " 'vanessa',\n",
       " 'angel',\n",
       " 'david',\n",
       " 'keith',\n",
       " 'and',\n",
       " 'john',\n",
       " 'rhys',\n",
       " 'davies',\n",
       " 'and',\n",
       " 'the',\n",
       " 'much',\n",
       " 'better',\n",
       " 'bc',\n",
       " 'by',\n",
       " 'roland',\n",
       " 'emmerich',\n",
       " 'with',\n",
       " 'with',\n",
       " 'steven',\n",
       " 'strait',\n",
       " 'cliff',\n",
       " 'curtis',\n",
       " 'and',\n",
       " 'camilla',\n",
       " 'belle',\n",
       " 'this',\n",
       " 'motion',\n",
       " 'picture',\n",
       " 'filled',\n",
       " 'with',\n",
       " 'bloody',\n",
       " 'moments',\n",
       " 'is',\n",
       " 'badly',\n",
       " 'directed',\n",
       " 'by',\n",
       " 'george',\n",
       " 'miller',\n",
       " 'and',\n",
       " 'with',\n",
       " 'no',\n",
       " 'originality',\n",
       " 'because',\n",
       " 'takes',\n",
       " 'too',\n",
       " 'many',\n",
       " 'elements',\n",
       " 'from',\n",
       " 'previous',\n",
       " 'films',\n",
       " 'miller',\n",
       " 'is',\n",
       " 'an',\n",
       " 'australian',\n",
       " 'director',\n",
       " 'usually',\n",
       " 'working',\n",
       " 'for',\n",
       " 'television',\n",
       " 'tidal',\n",
       " 'wave',\n",
       " 'journey',\n",
       " 'to',\n",
       " 'the',\n",
       " 'center',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'and',\n",
       " 'many',\n",
       " 'others',\n",
       " 'and',\n",
       " 'occasionally',\n",
       " 'for',\n",
       " 'cinema',\n",
       " 'the',\n",
       " 'man',\n",
       " 'from',\n",
       " 'snowy',\n",
       " 'river',\n",
       " 'zeus',\n",
       " 'and',\n",
       " 'roxanne',\n",
       " 'robinson',\n",
       " 'crusoe',\n",
       " 'rating',\n",
       " 'below',\n",
       " 'average',\n",
       " 'bottom',\n",
       " 'of',\n",
       " 'barrel']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.\n",
    "review3_words = review3.lower().split()\n",
    "review3_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "film\n",
      "start\n",
      "with\n",
      "a\n",
      "manag\n",
      "nichola\n",
      "bell\n",
      "give\n",
      "welcom\n",
      "investor\n",
      "robert\n",
      "carradin\n",
      "to\n",
      "primal\n",
      "park\n",
      "a\n",
      "secret\n",
      "project\n",
      "mutat\n",
      "a\n",
      "primal\n",
      "anim\n",
      "use\n",
      "fossil\n",
      "dna\n",
      "like\n",
      "jurassik\n",
      "park\n",
      "and\n",
      "some\n",
      "scientist\n",
      "resurrect\n",
      "one\n",
      "of\n",
      "natur\n",
      "s\n",
      "most\n",
      "fearsom\n",
      "predat\n",
      "the\n",
      "sabretooth\n",
      "tiger\n",
      "or\n",
      "smilodon\n",
      "scientif\n",
      "ambit\n",
      "turn\n",
      "deadli\n",
      "howev\n",
      "and\n",
      "when\n",
      "the\n",
      "high\n",
      "voltag\n",
      "fenc\n",
      "is\n",
      "open\n",
      "the\n",
      "creatur\n",
      "escap\n",
      "and\n",
      "begin\n",
      "savag\n",
      "stalk\n",
      "it\n",
      "prey\n",
      "the\n",
      "human\n",
      "visitor\n",
      "tourist\n",
      "and\n",
      "scientif\n",
      "meanwhil\n",
      "some\n",
      "youngster\n",
      "enter\n",
      "in\n",
      "the\n",
      "restrict\n",
      "area\n",
      "of\n",
      "the\n",
      "secur\n",
      "center\n",
      "and\n",
      "are\n",
      "attack\n",
      "by\n",
      "a\n",
      "pack\n",
      "of\n",
      "larg\n",
      "pre\n",
      "histor\n",
      "anim\n",
      "which\n",
      "are\n",
      "deadlier\n",
      "and\n",
      "bigger\n",
      "in\n",
      "addit\n",
      "a\n",
      "secur\n",
      "agent\n",
      "staci\n",
      "haiduk\n",
      "and\n",
      "her\n",
      "mate\n",
      "brian\n",
      "wimmer\n",
      "fight\n",
      "hardli\n",
      "against\n",
      "the\n",
      "carnivor\n",
      "smilodon\n",
      "the\n",
      "sabretooth\n",
      "themselv\n",
      "of\n",
      "cours\n",
      "are\n",
      "the\n",
      "real\n",
      "star\n",
      "star\n",
      "and\n",
      "they\n",
      "are\n",
      "astound\n",
      "terrifyingli\n",
      "though\n",
      "not\n",
      "convinc\n",
      "the\n",
      "giant\n",
      "anim\n",
      "savag\n",
      "are\n",
      "stalk\n",
      "it\n",
      "prey\n",
      "and\n",
      "the\n",
      "group\n",
      "run\n",
      "afoul\n",
      "and\n",
      "fight\n",
      "against\n",
      "one\n",
      "natur\n",
      "s\n",
      "most\n",
      "fearsom\n",
      "predat\n",
      "furthermor\n",
      "a\n",
      "third\n",
      "sabretooth\n",
      "more\n",
      "danger\n",
      "and\n",
      "slow\n",
      "stalk\n",
      "it\n",
      "victim\n",
      "the\n",
      "movi\n",
      "deliv\n",
      "the\n",
      "good\n",
      "with\n",
      "lot\n",
      "of\n",
      "blood\n",
      "and\n",
      "gore\n",
      "as\n",
      "behead\n",
      "hair\n",
      "rais\n",
      "chill\n",
      "full\n",
      "of\n",
      "scare\n",
      "when\n",
      "the\n",
      "sabretooth\n",
      "appear\n",
      "with\n",
      "mediocr\n",
      "special\n",
      "effect\n",
      "the\n",
      "stori\n",
      "provid\n",
      "excit\n",
      "and\n",
      "stir\n",
      "entertain\n",
      "but\n",
      "it\n",
      "result\n",
      "to\n",
      "be\n",
      "quit\n",
      "bore\n",
      "the\n",
      "giant\n",
      "anim\n",
      "are\n",
      "major\n",
      "made\n",
      "by\n",
      "comput\n",
      "gener\n",
      "and\n",
      "seem\n",
      "total\n",
      "lousi\n",
      "middl\n",
      "perform\n",
      "though\n",
      "the\n",
      "player\n",
      "react\n",
      "appropri\n",
      "to\n",
      "becom\n",
      "food\n",
      "actor\n",
      "give\n",
      "vigor\n",
      "physic\n",
      "perform\n",
      "dodg\n",
      "the\n",
      "beast\n",
      "run\n",
      "bound\n",
      "and\n",
      "leap\n",
      "or\n",
      "dangl\n",
      "over\n",
      "wall\n",
      "and\n",
      "it\n",
      "pack\n",
      "a\n",
      "ridicul\n",
      "final\n",
      "deadli\n",
      "scene\n",
      "no\n",
      "for\n",
      "small\n",
      "kid\n",
      "by\n",
      "realist\n",
      "gori\n",
      "and\n",
      "violent\n",
      "attack\n",
      "scene\n",
      "other\n",
      "film\n",
      "about\n",
      "sabretooth\n",
      "or\n",
      "smilodon\n",
      "are\n",
      "the\n",
      "follow\n",
      "sabretooth\n",
      "by\n",
      "jame\n",
      "r\n",
      "hickox\n",
      "with\n",
      "vanessa\n",
      "angel\n",
      "david\n",
      "keith\n",
      "and\n",
      "john\n",
      "rhi\n",
      "davi\n",
      "and\n",
      "the\n",
      "much\n",
      "better\n",
      "bc\n",
      "by\n",
      "roland\n",
      "emmerich\n",
      "with\n",
      "with\n",
      "steven\n",
      "strait\n",
      "cliff\n",
      "curti\n",
      "and\n",
      "camilla\n",
      "bell\n",
      "thi\n",
      "motion\n",
      "pictur\n",
      "fill\n",
      "with\n",
      "bloodi\n",
      "moment\n",
      "is\n",
      "badli\n",
      "direct\n",
      "by\n",
      "georg\n",
      "miller\n",
      "and\n",
      "with\n",
      "no\n",
      "origin\n",
      "becaus\n",
      "take\n",
      "too\n",
      "mani\n",
      "element\n",
      "from\n",
      "previou\n",
      "film\n",
      "miller\n",
      "is\n",
      "an\n",
      "australian\n",
      "director\n",
      "usual\n",
      "work\n",
      "for\n",
      "televis\n",
      "tidal\n",
      "wave\n",
      "journey\n",
      "to\n",
      "the\n",
      "center\n",
      "of\n",
      "the\n",
      "earth\n",
      "and\n",
      "mani\n",
      "other\n",
      "and\n",
      "occasion\n",
      "for\n",
      "cinema\n",
      "the\n",
      "man\n",
      "from\n",
      "snowi\n",
      "river\n",
      "zeu\n",
      "and\n",
      "roxann\n",
      "robinson\n",
      "cruso\n",
      "rate\n",
      "below\n",
      "averag\n",
      "bottom\n",
      "of\n",
      "barrel\n"
     ]
    }
   ],
   "source": [
    "#5.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() #creates a new porter stemmer\n",
    "for word in review3_words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStemmer groups the same class of words into the same category as the stem word. For example, \"waits\", \"waited\", and \"waiting\" are all categorized under its stem \"wait\" and \"games\", \"gamed\", and \"gaming\" are grouped into \"game\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to Part Of Speech (POS)) tag the third movie review. We will use POS labeling also called grammatical tagging. To do this import from nltk.tag import pos_tag. When you use pos_tag on a word it returns a token-tag pair in the form of a tuple. In NLTK's Penn Treebank POS, the abbreviation (tag) for an adjective is JJ and NN for singular nouns. Count the number of singular nouns (NN) and adjectives (JJ) in review3_words using NLTK's pos_tag_sents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of singular nouns (NN): 75\n",
      "Number of adjectives (JJ): 48\n"
     ]
    }
   ],
   "source": [
    "#6.\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "word_tag = nltk.tag.pos_tag(review3_words)\n",
    "counts = Counter(tag for word,tag in word_tag)\n",
    "num_nn = counts['NN']\n",
    "num_jj = counts['JJ']\n",
    "print('Number of singular nouns (NN):', num_nn)\n",
    "print('Number of adjectives (JJ):', num_jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P. STEMMER: gone fish earlier than suppos to. hi shirt were damag\n",
      "\n",
      "LEMMATIZER: go fish early than suppose to. his shirt be damage\n",
      "\n",
      "LEMMATIZER: film start manager nicholas bell give welcome investor robert carradine primal park secret project mutate primal animal use fossilized dna like jurassik park scientist resurrect one nature fearsome predator sabretooth tiger smilodon scientific ambition turn deadly however high voltage fence open creature escape begin savagely stalk prey human visitor tourist scientific meanwhile youngster enter restrict area security center attack pack large pre historical animal deadlier big addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star star astound terrifyingly though convince giant animal savagely stalk prey group run afoul fight one nature fearsome predator furthermore third sabretooth dangerous slow stalk victim movie delivers good lot blood gore behead hair raise chill full scare sabretooths appear mediocre special effect story provide excite stir entertainment result quite bore giant animal majority make computer generator seem totally lousy middle performance though player react appropriately become food actor give vigorously physical performance dodge beast run bound leap dangle wall pack ridiculous final deadly scene small kid realistic gory violent attack scene film sabretooths smilodon follow sabretooth james r hickox vanessa angel david keith john rhys davy much well bc roland emmerich steven strait cliff curtis camilla belle motion picture fill bloody moment badly direct george miller originality take many element previous film miller australian director usually work television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel\n"
     ]
    }
   ],
   "source": [
    "## 7. Example code for lemmatizing, extend it to work on the third movie review ##\n",
    "\n",
    "example_sentence = 'gone fishing earlier than supposed to. his shirts were damaged'\n",
    "\n",
    "# compare to porter stemmer (base case)\n",
    "ps = PorterStemmer()\n",
    "print('P. STEMMER:',' '.join([ps.stem(w) for w in example_sentence.split()]))\n",
    "\n",
    "# Lemmatizing\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''Treebank to wordnet POS tag'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n' #basecase POS\n",
    "    \n",
    "token_tag = pos_tag(example_sentence.split())\n",
    "print('\\nLEMMATIZER:',' '.join([wnl.lemmatize(w, pos=get_wordnet_pos(t)) for w, t in token_tag]))\n",
    "\n",
    "print('\\nLEMMATIZER:',' '.join([wnl.lemmatize(w ,pos=get_wordnet_pos(t)) for w, t in word_tag]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review3_words without stopwords: ['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', 'jurassik', 'park', 'scientists', 'resurrect', 'one', 'nature', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'however', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'scientific', 'meanwhile', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'pre', 'historical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'though', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'one', 'nature', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalks', 'victims', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hair', 'raising', 'chills', 'full', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'effects', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'made', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middling', 'performances', 'though', 'players', 'reacting', 'appropriately', 'becoming', 'food', 'actors', 'give', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'running', 'bound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realistic', 'gory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', 'sabretooth', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'much', 'better', 'bc', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'many', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'many', 'others', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxanne', 'robinson', 'crusoe', 'rating', 'average', 'bottom', 'barrel']\n",
      "Number of stopwords removed: 135\n",
      "Review3_words stopwords: ['the', 'with', 'a', 'to', 'a', 'a', 'and', 'some', 'of', 's', 'most', 'the', 'or', 'and', 'when', 'the', 'is', 'the', 'and', 'its', 'the', 'and', 'some', 'in', 'the', 'of', 'the', 'and', 'are', 'by', 'a', 'of', 'which', 'are', 'and', 'in', 'a', 'and', 'her', 'against', 'the', 'the', 'themselves', 'of', 'are', 'the', 'and', 'they', 'are', 'not', 'the', 'are', 'its', 'and', 'the', 'and', 'against', 's', 'most', 'a', 'more', 'and', 'its', 'the', 'the', 'with', 'of', 'and', 'as', 'of', 'when', 'the', 'with', 'the', 'and', 'but', 'it', 'to', 'be', 'the', 'are', 'by', 'and', 'the', 'to', 'the', 'and', 'or', 'over', 'and', 'it', 'a', 'no', 'for', 'by', 'and', 'other', 'about', 'or', 'are', 'the', 'by', 'with', 'and', 'and', 'the', 'by', 'with', 'with', 'and', 'this', 'with', 'is', 'by', 'and', 'with', 'no', 'because', 'too', 'from', 'is', 'an', 'for', 'to', 'the', 'of', 'the', 'and', 'and', 'for', 'the', 'from', 'and', 'below', 'of']\n"
     ]
    }
   ],
   "source": [
    "#8.\n",
    "eng_stopwords = stopwords.words('english')\n",
    "review3_stopwords = [word for word in review3_words if word in eng_stopwords]\n",
    "review3_words = [word for word in review3_words if word not in eng_stopwords]\n",
    "print('Review3_words without stopwords:', review3_words)\n",
    "print('Number of stopwords removed:', len(review3_stopwords))\n",
    "print('Review3_words stopwords:', review3_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:3\n",
    "\n",
    "* 1: Create a function called `review_cleaner` that reads in a review and\n",
    "    - Removes HTML tags (using beautifulsoup)\n",
    "    - Removes non-letters (using regular expression)\n",
    "    - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "    - Removes all the English stopwords from the list of movie review words\n",
    "    - Joins the words back into one string separated by space\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**\n",
    "    \n",
    "    \n",
    "* 2: Create three lists: \n",
    "    - `review_clean_original`, `review_clean_ps` and `review_clean_wnl`. Where `review_clean_original` contains all the reviews from the train DataFrame, that have been cleaned by the function `review_cleaner` defined in 1.\n",
    "    - `review_clean_ps` applies the PorterStemmer to the reviews in `review_clean_original`. **Note:** NLTK version 3.2.2 crashes when trying to use the PorterStemming on the string 'oed' (known bug). Therefore, use an if statement to skip just that specific string/word.\n",
    "    - `review_clean_wnl` contains words that have been lemmatized using NLTK's WordNetLemmatizer on the words in the list `review_clean_original`.\n",
    "    \n",
    "**Note, problem 2: can take more than 10minutes to run on a laptop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Part 1\n",
    "\n",
    "def review_cleaner(review):\n",
    "    ## EXTEND THIS FUNCTION SO THAT IT COMPLETES THE FOLLOWING STEPS: ##\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1.\n",
    "    review = bs.BeautifulSoup(review, features='lxml').text\n",
    "    \n",
    "    #2.\n",
    "    review = re.sub('[^a-zA-Z]',' ', review)\n",
    "    \n",
    "    #3.\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #4. CHECK FOR STOPWORDS IN: eng_stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #5. \n",
    "    review = ' '.join(word[0] for word in review)\n",
    "    \n",
    "    #6.\n",
    "    return(review)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews for review_clean_original\n",
      "Done with 1000 reviews for review_clean_original\n",
      "Done with 1500 reviews for review_clean_original\n",
      "Done with 2000 reviews for review_clean_original\n",
      "Done with 2500 reviews for review_clean_original\n",
      "Done with 3000 reviews for review_clean_original\n",
      "Done with 3500 reviews for review_clean_original\n",
      "Done with 4000 reviews for review_clean_original\n",
      "Done with 4500 reviews for review_clean_original\n",
      "Done with 5000 reviews for review_clean_original\n",
      "Done with 5500 reviews for review_clean_original\n",
      "Done with 6000 reviews for review_clean_original\n",
      "Done with 6500 reviews for review_clean_original\n",
      "Done with 7000 reviews for review_clean_original\n",
      "Done with 7500 reviews for review_clean_original\n",
      "Done with 8000 reviews for review_clean_original\n",
      "Done with 8500 reviews for review_clean_original\n",
      "Done with 9000 reviews for review_clean_original\n",
      "Done with 9500 reviews for review_clean_original\n",
      "Done with 10000 reviews for review_clean_original\n",
      "Done with 10500 reviews for review_clean_original\n",
      "Done with 11000 reviews for review_clean_original\n",
      "Done with 11500 reviews for review_clean_original\n",
      "Done with 12000 reviews for review_clean_original\n",
      "Done with 12500 reviews for review_clean_original\n",
      "Done with 13000 reviews for review_clean_original\n",
      "Done with 13500 reviews for review_clean_original\n",
      "Done with 14000 reviews for review_clean_original\n",
      "Done with 14500 reviews for review_clean_original\n",
      "Done with 15000 reviews for review_clean_original\n",
      "Done with 15500 reviews for review_clean_original\n",
      "Done with 16000 reviews for review_clean_original\n",
      "Done with 16500 reviews for review_clean_original\n",
      "Done with 17000 reviews for review_clean_original\n",
      "Done with 17500 reviews for review_clean_original\n",
      "Done with 18000 reviews for review_clean_original\n",
      "Done with 18500 reviews for review_clean_original\n",
      "Done with 19000 reviews for review_clean_original\n",
      "Done with 19500 reviews for review_clean_original\n",
      "Done with 20000 reviews for review_clean_original\n",
      "Done with 20500 reviews for review_clean_original\n",
      "Done with 21000 reviews for review_clean_original\n",
      "Done with 21500 reviews for review_clean_original\n",
      "Done with 22000 reviews for review_clean_original\n",
      "Done with 22500 reviews for review_clean_original\n",
      "Done with 23000 reviews for review_clean_original\n",
      "Done with 23500 reviews for review_clean_original\n",
      "Done with 24000 reviews for review_clean_original\n",
      "Done with 24500 reviews for review_clean_original\n",
      "Done with 25000 reviews for review_clean_original\n",
      "Done with 500 reviews for review_clean_original\n",
      "Done with 1000 reviews for review_clean_original\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-f1b19c9011b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mreview_clean_ps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview_clean_original\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m## Step3: Use review_clean original to create review_clean_wnl using the WordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-f1b19c9011b5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mreview_clean_ps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview_clean_original\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m## Step3: Use review_clean original to create review_clean_wnl using the WordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Part 2\n",
    "\n",
    "## Step 1: Clean up all the original reviews\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews for review_clean_original\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))\n",
    "    \n",
    "\n",
    "## Step2: Use review_clean_original to create review_clean_ps using PorterStemmer\n",
    "#review_clean_ps applies the PorterStemmer to the reviews in review_clean_original\n",
    "review_clean_ps = []\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews for review_clean_original\" %(i+1))\n",
    "    \n",
    "    for w in review_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "    review_clean_ps.append(' '.join([ps.stem(w) for w in review_clean_original[i].split()]))\n",
    "\n",
    "## Step3: Use review_clean original to create review_clean_wnl using the WordNetLemmatizer\n",
    "#review_clean_wnl contains words that have been lemmatized using NLTK's WordNetLemmatizer on the words in the list\n",
    "#review_clean_original\n",
    "review_clean_wnl = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:4: Feature vectors and Bag of Words model\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Derived from source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "We will now use scikit-learn to create numeric representations of the words in the reviews, using a method called Bag of Words. You can see this as learning a vocabulary from all the reviews and counting how many times a word appears in the reviews. For example, if we have two sentences:\n",
    "\n",
    "**Sentence 1:** \"cool students study cool data science\"\n",
    "\n",
    "**Sentence 2:** \"to know data science study data science\"\n",
    "\n",
    "The vocabulary of these two sentences can be summarized in a dictionary:\n",
    "\n",
    "{ cool, students, study, data, science, to, know }\n",
    "\n",
    "The bags of words count the number of times each word occur in a sentence. In Sentence 1, \"cool\" appears twice, and \"students\", \"study\", \"data\", and \"science\" appear once. The feature vector for Sentence 1 is:\n",
    "\n",
    "Sentence 1: { 2, 1, 1, 1, 1, 0, 0 }\n",
    "\n",
    "And for Sentence 2: { 0, 0, 1, 2, 2, 1, 1}\n",
    "\n",
    "### Applying this strategy to the IMDB movie reviews\n",
    "\n",
    "The movie review data contains a lot of words. To limit the analysis we use the 5000 most frequent words from the cleaned reviews. To extract the bag of words features we will use scitkit-learn.\n",
    "\n",
    "The training data will be created by the `CountVectorizer` function from scikit-learn, and the training array will have 25000 rows (one for each review) and 5000 features (one for each vocabulary word).\n",
    "\n",
    "CountVectorizer can automatically handle text cleaning, but here we specify \"None\", instead we did a step-by-step cleaning of the data in the earlier problems.\n",
    "\n",
    "#### Random Forest for review sentiment classification\n",
    "\n",
    "First split up the data set so that 80% are used as training samples (the first 20000 reviews and their sentiment) and 20% are used as validation samples (the last 5000 reviews and their sentiment). Use Random Forest to do numeric training on the features for the training samples from the Bag of Words and their respective sentiment labels for each review / feature vector. The number of trees is set to 50 as a default value.\n",
    "\n",
    "## Problem\n",
    "\n",
    "* 1: Run this analysis for the three cleaned review lists, i.e. `review_clean_original`, `review_clean_ps` and `review_clean_wnl`, by using the code below. Extend the function to print the **validation accuracy** by using `forest.predict(train_data_features[20000:,:])` and then comparing the resulting sentiment predictions with the ones stored in `train[\"sentiment\"][20000:]`. **Note:** The printed validation accuracy should show the percentage of correctly predicted sentiments for the validation set.\n",
    "\n",
    "\n",
    "* 2: Print the validation accuracy obtained for the three models. **Note:** Takes about 4mins to run.\n",
    "\n",
    "\n",
    "* 3: What data preprocessing strategy worked the best? Why do you think that is? (Feel free to change the number of features extracted in the bag of words model and the number of trees in the random forest model, to see how it effects your accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Can't use statement directly after '%%time'!\n"
     ]
    }
   ],
   "source": [
    "%%time # times the operation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def predict_sentiment(X,y=train[\"sentiment\"]):\n",
    "    \n",
    "    \n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool.\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 5000) \n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    train_data_features = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "    \n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_data_features[0:20000,:], y[0:20000] )\n",
    "    \n",
    "    \n",
    "    '''## MAKE PREDICTIONS HERE ##'''\n",
    "    \n",
    "    y=train[\"sentiment\"]\n",
    "    ....\n",
    "    valid_predictions = forest.predict(train_data_features[20000:,:])\n",
    "    valid_acc = np.mean(valid_predictions == y[20000:])\n",
    "    \n",
    "'''Then run'''\n",
    "predict_sentiment(review_clean_original)\n",
    "predict_sentiment(review_clean_ps)\n",
    "predict_sentiment(review_clean_wnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extra Credit (worth 1p)\n",
    "\n",
    "* **Question:** Preprocess the reviews in any way you find suitable and build your own ML model that can predict the sentiment of movie reviews. Credit will be given if you can obtain a prediction accuracy of over 90%, when predicting the sentiments of the validation set (the last 5000 reviews). Train your model on the first 20000 reviews (with their sentiment as the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Input Answer ##"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
